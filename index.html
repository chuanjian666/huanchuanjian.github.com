<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>datahacker</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="专注大数据平台，数据分析，数据挖掘等技术">
<meta name="keywords" content="spark, streaming">
<meta property="og:type" content="website">
<meta property="og:title" content="datahacker">
<meta property="og:url" content="http://datahacker.me/index.html">
<meta property="og:site_name" content="datahacker">
<meta property="og:description" content="专注大数据平台，数据分析，数据挖掘等技术">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="datahacker">
<meta name="twitter:description" content="专注大数据平台，数据分析，数据挖掘等技术">
  
    <link rel="alternative" href="/atom.xml" title="datahacker" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
</head></html>
<body>
  <div id="container">
    <div class="mobile-nav-panel">
	<i class="icon-reorder icon-large"></i>
</div>
<header id="header">
	<h1 class="blog-title">
		<a href="/">datahacker</a>
	</h1>
	<nav class="nav">
		<ul>
			<li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li>
			<li><a id="nav-search-btn" class="nav-icon" title="Search"></a></li>
			<li><a href="/atom.xml" id="nav-rss-link" class="nav-icon" title="RSS Feed"></a></li>
		</ul>
	</nav>
	<div id="search-form-wrap">
		<form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://datahacker.me"></form>
	</div>
</header>
    <div id="main">
      
  
    <article id="post-spark-sql-streaming-performance-profile" class="post">
	<footer class="entry-meta-header">
		<span class="meta-elements date">
			<a href="/2019/01/22/spark-sql-streaming-performance-profile/" class="article-date">
  <time datetime="2019-01-22T02:18:10.000Z" itemprop="datePublished">2019-01-22</time>
</a>
		</span>
		<span class="meta-elements author">huanzh</span>
		<div class="commentscount">
			
		</div>
	</footer>
	
	<header class="entry-header">
		
  
    <h1 itemprop="name" class="entry-title">
      <a class="article-title" href="/2019/01/22/spark-sql-streaming-performance-profile/">Spark Structured Streaming 性能诊断</a>
    </h1>
  

	</header>
	<div class="entry-content">
		
    	<p>最近组内同事和我反馈，我提交到集群上的一个实时计算 Job 资源占用较高，而该 Job 处理数据量不大，所以怀疑有性能问题。</p>
<p>打开 Spark 应用监控后台如下图：</p>
<p><img src="streaming-job-ui.png" alt="image"></p>
<p>目前我们处理实时计算都是基于 Structured Streaming，本质上是一个个 <code>Micro Batch</code>，挑了其中一个 <code>Batch Job</code> 发现被拆分为 2 个 Stages，说明其中发生了一次 <code>Shuffle</code>，然后 Stage1 的 Tasks 数为 5，和 Source 的 Kafka topic 分区数一致，Stage2 的 Tasks 数居然是 300， 问题找到了：</p>
<p><strong>Shuffle Partitions 过大，相应的分配给 Stage2 的 Task 数过多，导致资源占用过高</strong></p>
<p>原因找到了那就调整配置，修改 shuffle partition 的配置项为 <code>spark.sql.shuffle.partitions</code>，指定的方式也有几种：</p>
<ul>
<li>driver 程序中创建 SparkSession 时指定</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.</span><br><span class="line">      appName(<span class="string">"App Name xxxx"</span>).</span><br><span class="line">      config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).</span><br><span class="line">      getOrCreate()</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 spark-submit 提交任务时指定</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>&#123;SPARK_HOME&#125;/bin/spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --conf spark.sql.shuffle.partitions=2 \</span><br><span class="line">    --queue queueName \</span><br><span class="line">    --packages packageName \</span><br><span class="line">    --class ClassName /path/to/xxxx.jar \</span><br></pre></td></tr></table></figure>
<p>修改后重新提交 Job，发现问题：</p>
<p><strong>监控后台 Environment 显示配置项值已经为 2，但是 Stage2 的 Task 数依旧为300，说明配置没有生效</strong></p>
<p><img src="spark-prop.png" alt="image"></p>
<p>google 了一圈没有找到相关的资料，只好翻了下 Structured Streaming 相关的源码，发现比较有意思的几点：</p>
<p>其中 <code>MicroBatchExecution.scala</code> 继承自 <code>StreamExecution.scala</code> 抽象类，管理着 <code>Structured Streaming</code> 这种 <code>Micro-Batch</code> 方式的执行逻辑，与之对应还有 <code>Continuous</code> 方式，相应的子类是 <code>ContinuousExecution.scala</code>，关于 <code>Continuous</code> 方式这里不做详细描述。</p>
<p>阅读 <code>MicroBatchExecution.scala -&gt; runActivatedStream</code> 方法，其中有一段初始化逻辑如下：</p>
<p><img src="microbatch-runstream.png" alt="image"></p>
<p><code>currentBatchId</code> 默认为 -1，每次处理完一个 batch 后 +1，继续阅读 <code>populateStartOffsets</code> 方法</p>
<p><img src="microbatch-populate.png" alt="image"></p>
<p><code>populateStartOffsets</code> 方法会用上一次 batch 的元数据更新下一次 batch 的 <code>SparkSession Config</code>，填充逻辑在 <code>OffsetSeqMetadata.setSessionConf(metadata, sparkSessionToRunBatches.conf)</code> 中</p>
<p><img src="offsetseq-setconf.png" alt="image"></p>
<p>到此，配置不生效的原因已经清楚了：</p>
<p><strong>初次执行时没有设置该配置项，导致使用系统默认值 300，后续的 batch 每次都延续上一个 batch 的元数据，所以始终都会是 300</strong></p>
<p>查看下 <code>HDFS</code> 上元数据也印证了以上结论</p>
<p><img src="streaming-meta.png" alt="image"></p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>重新指定 <code>checkpoint</code> 路径，重新提交后配置生效，Tasks 数由 305 (5 + 300) 降至 7 (5 + 2)，Job 跑的如丝般顺滑 :)</p>
<p><img src="result.png" alt="image"></p>

    
	</div>
	<footer class="entry-footer">
		<div class="entry-meta-footer">
			<span class="category">
				
			</span>
			<span class="tags">
				
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL-Streaming-Performance/">Spark SQL, Streaming, Performance</a></li></ul>

			</span>
		</div>
	</footer>
	
</article>


	<hr class="article-devider">



  
    <article id="post-spark-sql-streaming-monitor-alert" class="post">
	<footer class="entry-meta-header">
		<span class="meta-elements date">
			<a href="/2018/10/22/spark-sql-streaming-monitor-alert/" class="article-date">
  <time datetime="2018-10-22T03:21:47.000Z" itemprop="datePublished">2018-10-22</time>
</a>
		</span>
		<span class="meta-elements author">huanzh</span>
		<div class="commentscount">
			
		</div>
	</footer>
	
	<header class="entry-header">
		
  
    <h1 itemprop="name" class="entry-title">
      <a class="article-title" href="/2018/10/22/spark-sql-streaming-monitor-alert/">Spark Structured Streaming 实现监控告警</a>
    </h1>
  

	</header>
	<div class="entry-content">
		
    	<h4 id="实现-StreamingQueryListener-抽象类"><a href="#实现-StreamingQueryListener-抽象类" class="headerlink" title="实现 StreamingQueryListener 抽象类"></a>实现 StreamingQueryListener 抽象类</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaMetrics</span>(<span class="params">servers: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">StreamingQueryListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> kafkaProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">  kafkaProperties.put(<span class="string">"bootstrap.servers"</span>, servers)</span><br><span class="line">  kafkaProperties.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">  kafkaProperties.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](kafkaProperties)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onQueryStarted</span></span>(event: <span class="type">StreamingQueryListener</span>.<span class="type">QueryStartedEvent</span>): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onQueryProgress</span></span>(event: <span class="type">StreamingQueryListener</span>.<span class="type">QueryProgressEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>(<span class="string">"streamingMetrics"</span>, event.progress.json))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onQueryTerminated</span></span>(event: <span class="type">StreamingQueryListener</span>.<span class="type">QueryTerminatedEvent</span>): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="添加-Listener"><a href="#添加-Listener" class="headerlink" title="添加 Listener"></a>添加 Listener</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streams.addListener(<span class="keyword">new</span> <span class="type">KafkaMetrics</span>(<span class="string">"kafkaAddress"</span>))</span><br></pre></td></tr></table></figure>
<h4 id="通过实现-kafka-消费者程序-将监控数据写入-HDFS-使用公共-dataPipeline-数据结构如下"><a href="#通过实现-kafka-消费者程序-将监控数据写入-HDFS-使用公共-dataPipeline-数据结构如下" class="headerlink" title="通过实现 kafka 消费者程序, 将监控数据写入 HDFS, 使用公共 dataPipeline, 数据结构如下"></a>通过实现 kafka 消费者程序, 将监控数据写入 HDFS, 使用公共 dataPipeline, 数据结构如下</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"id"</span>:<span class="string">"ee6a297a-07c6-4d59-97b2-e8b5f3a3a240"</span>,</span><br><span class="line">    <span class="attr">"runId"</span>:<span class="string">"58cae410-0208-4644-99f2-ba2cd9c3b5c5"</span>,</span><br><span class="line">    <span class="attr">"name"</span>:<span class="string">"query_zhx_pv_action"</span>,</span><br><span class="line">    <span class="attr">"timestamp"</span>:<span class="string">"2018-11-22T01:38:14.041Z"</span>,</span><br><span class="line">    <span class="attr">"batchId"</span>:<span class="number">493</span>,</span><br><span class="line">    <span class="attr">"numInputRows"</span>:<span class="number">1986</span>,</span><br><span class="line">    <span class="attr">"processedRowsPerSecond"</span>:<span class="number">253.5103395455706</span>,</span><br><span class="line">    <span class="attr">"durationMs"</span>:&#123;</span><br><span class="line">        <span class="attr">"addBatch"</span>:<span class="number">6783</span>,</span><br><span class="line">        <span class="attr">"getBatch"</span>:<span class="number">60</span>,</span><br><span class="line">        <span class="attr">"getOffset"</span>:<span class="number">228</span>,</span><br><span class="line">        <span class="attr">"queryPlanning"</span>:<span class="number">371</span>,</span><br><span class="line">        <span class="attr">"triggerExecution"</span>:<span class="number">7834</span>,</span><br><span class="line">        <span class="attr">"walCommit"</span>:<span class="number">116</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"eventTime"</span>:&#123;</span><br><span class="line">        <span class="attr">"avg"</span>:<span class="string">"2018-11-21T05:45:08.262Z"</span>,</span><br><span class="line">        <span class="attr">"max"</span>:<span class="string">"2018-11-22T01:31:58.202Z"</span>,</span><br><span class="line">        <span class="attr">"min"</span>:<span class="string">"2018-11-20T09:25:45.249Z"</span>,</span><br><span class="line">        <span class="attr">"watermark"</span>:<span class="string">"2018-11-18T09:18:23.404Z"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"stateOperators"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"numRowsTotal"</span>:<span class="number">56</span>,</span><br><span class="line">            <span class="attr">"numRowsUpdated"</span>:<span class="number">47</span>,</span><br><span class="line">            <span class="attr">"memoryUsedBytes"</span>:<span class="number">37583</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"sources"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"description"</span>:<span class="string">"KafkaSource[Subscribe[topicBuryPoint-3]]"</span>,</span><br><span class="line">            <span class="attr">"startOffset"</span>:&#123;</span><br><span class="line">                <span class="attr">"topicBuryPoint-3"</span>:&#123;</span><br><span class="line">                    <span class="attr">"0"</span>:<span class="number">30880</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"endOffset"</span>:&#123;</span><br><span class="line">                <span class="attr">"topicBuryPoint-3"</span>:&#123;</span><br><span class="line">                    <span class="attr">"0"</span>:<span class="number">32866</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"numInputRows"</span>:<span class="number">1986</span>,</span><br><span class="line">            <span class="attr">"processedRowsPerSecond"</span>:<span class="number">253.5103395455706</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"sink"</span>:&#123;</span><br><span class="line">        <span class="attr">"description"</span>:<span class="string">"org.apache.spark.sql.kafka010.KafkaSourceProvider@6837dfea"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="使用-Spark-创建分区和视图-在-redash-里通过-sql-查询并制作图表"><a href="#使用-Spark-创建分区和视图-在-redash-里通过-sql-查询并制作图表" class="headerlink" title="使用 Spark 创建分区和视图, 在 redash 里通过 sql 查询并制作图表"></a>使用 Spark 创建分区和视图, 在 redash 里通过 sql 查询并制作图表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">date_format</span>(<span class="built_in">timestamp</span>, <span class="string">'HH'</span>) <span class="keyword">as</span> <span class="keyword">hour</span>, </span><br><span class="line">    <span class="keyword">avg</span>(inputRowsPerSecond) <span class="keyword">as</span> inputRowsPerSecondAvg, </span><br><span class="line">    <span class="keyword">avg</span>(processedRowsPerSecond) <span class="keyword">as</span> processedRowsPerSecondAvg,</span><br><span class="line">    <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> batchNum,</span><br><span class="line">    <span class="keyword">avg</span>(durationMs_getOffset) <span class="keyword">as</span> durationMs_getOffsetAvg,</span><br><span class="line">    <span class="keyword">avg</span>(durationMs_triggerExecution) <span class="keyword">as</span> durationMs_triggerExecutionAvg</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    ods.ods_fm_bizdata_streaming_metrics_view</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    <span class="keyword">date_format</span>(<span class="keyword">current_timestamp</span>(), <span class="string">'yyyy-MM-dd'</span>) = <span class="keyword">date_format</span>(<span class="built_in">timestamp</span>, <span class="string">'yyyy-MM-dd'</span>) </span><br><span class="line">    <span class="keyword">and</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">'query_order%'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">hour</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">hour</span> <span class="keyword">desc</span></span><br></pre></td></tr></table></figure>
<p><img src="http://pitxuq913.bkt.clouddn.com/streaming-metrics.png" alt="image"></p>
<h4 id="使用-redash-制作告警"><a href="#使用-redash-制作告警" class="headerlink" title="使用 redash 制作告警"></a>使用 redash 制作告警</h4><p><strong>配置定时任务: 每分钟查询前1分钟内的监控数据, 如果低于阈值则为服务异常</strong></p>
<p><strong>测试告警结果:-)</strong></p>
<p><img src="http://pitxuq913.bkt.clouddn.com/streaming-alert-mail.png" alt="image"></p>

    
	</div>
	<footer class="entry-footer">
		<div class="entry-meta-footer">
			<span class="category">
				
			</span>
			<span class="tags">
				
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL-Streaming-Monitor/">Spark SQL, Streaming, Monitor</a></li></ul>

			</span>
		</div>
	</footer>
	
</article>


	<hr class="article-devider">



  
  

    </div>
    <div class="mb-search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:datahacker.me">
  </form>
</div>
<footer id="footer">
	<h1 class="footer-blog-title">
		<a href="/">datahacker</a>
	</h1>
	<span class="copyright">
		&copy; 2019 huanzh<br>
		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	</span>
</footer>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div>
</body>
</html>
<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Spark Structured Streaming 性能诊断 | datahacker</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="最近组内同事和我反馈，我提交到集群上的一个实时计算 Job 资源占用较高，而该 Job 处理数据量不大，所以怀疑有性能问题。 打开 Spark 应用监控后台如下图：">
<meta name="keywords" content="Spark SQL, Streaming, Performance">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Structured Streaming 性能诊断">
<meta property="og:url" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/index.html">
<meta property="og:site_name" content="datahacker">
<meta property="og:description" content="最近组内同事和我反馈，我提交到集群上的一个实时计算 Job 资源占用较高，而该 Job 处理数据量不大，所以怀疑有性能问题。 打开 Spark 应用监控后台如下图：">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/streaming-job-ui.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/spark-prop.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/microbatch-runstream.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/microbatch-populate.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/offsetseq-setconf.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/streaming-meta.png">
<meta property="og:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/result.png">
<meta property="og:updated_time" content="2019-01-22T10:28:48.226Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Structured Streaming 性能诊断">
<meta name="twitter:description" content="最近组内同事和我反馈，我提交到集群上的一个实时计算 Job 资源占用较高，而该 Job 处理数据量不大，所以怀疑有性能问题。 打开 Spark 应用监控后台如下图：">
<meta name="twitter:image" content="http://datahacker.me/2019/01/22/spark-sql-streaming-performance-profile/streaming-job-ui.png">
  
    <link rel="alternative" href="/atom.xml" title="datahacker" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
</head></html>
<body>
  <div id="container">
    <div class="mobile-nav-panel">
	<i class="icon-reorder icon-large"></i>
</div>
<header id="header">
	<h1 class="blog-title">
		<a href="/">datahacker</a>
	</h1>
	<nav class="nav">
		<ul>
			<li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li>
			<li><a id="nav-search-btn" class="nav-icon" title="Search"></a></li>
			<li><a href="/atom.xml" id="nav-rss-link" class="nav-icon" title="RSS Feed"></a></li>
		</ul>
	</nav>
	<div id="search-form-wrap">
		<form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://datahacker.me"></form>
	</div>
</header>
    <div id="main">
      <article id="post-spark-sql-streaming-performance-profile" class="post">
	<footer class="entry-meta-header">
		<span class="meta-elements date">
			<a href="/2019/01/22/spark-sql-streaming-performance-profile/" class="article-date">
  <time datetime="2019-01-22T02:18:10.000Z" itemprop="datePublished">2019-01-22</time>
</a>
		</span>
		<span class="meta-elements author">huanzh</span>
		<div class="commentscount">
			
		</div>
	</footer>
	
	<header class="entry-header">
		
  
    <h1 class="article-title entry-title" itemprop="name">
      Spark Structured Streaming 性能诊断
    </h1>
  

	</header>
	<div class="entry-content">
		
    	<p>最近组内同事和我反馈，我提交到集群上的一个实时计算 Job 资源占用较高，而该 Job 处理数据量不大，所以怀疑有性能问题。</p>
<p>打开 Spark 应用监控后台如下图：</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/streaming-job-ui.png" title="spark-job-ui">
<a id="more"></a>
<p>目前我们处理实时计算都是基于 Structured Streaming，本质上是一个个 <code>Micro Batch</code>，挑了其中一个 <code>Batch Job</code> 发现被拆分为 2 个 Stages，说明其中发生了一次 <code>Shuffle</code>，然后 Stage1 的 Tasks 数为 5，和 Source 的 Kafka topic 分区数一致，Stage2 的 Tasks 数居然是 300， 问题找到了：</p>
<p><strong>Shuffle Partitions 过大，相应的分配给 Stage2 的 Task 数过多，导致资源占用过高</strong></p>
<p>原因找到了那就调整配置，修改 shuffle partition 的配置项为 <code>spark.sql.shuffle.partitions</code>，指定的方式也有几种：</p>
<ul>
<li>driver 程序中创建 SparkSession 时指定</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.</span><br><span class="line">      appName(<span class="string">"App Name xxxx"</span>).</span><br><span class="line">      config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).</span><br><span class="line">      getOrCreate()</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 spark-submit 提交任务时指定</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>&#123;SPARK_HOME&#125;/bin/spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --conf spark.sql.shuffle.partitions=2 \</span><br><span class="line">    --queue queueName \</span><br><span class="line">    --packages packageName \</span><br><span class="line">    --class ClassName /path/to/xxxx.jar \</span><br></pre></td></tr></table></figure>
<p>修改后重新提交 Job，发现问题：</p>
<p><strong>监控后台 Environment 显示配置项值已经为 2，但是 Stage2 的 Task 数依旧为300，说明配置没有生效</strong></p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/spark-prop.png" title="spark-prop">
<p>google 了一圈没有找到相关的资料，只好翻了下 Structured Streaming 相关的源码，发现比较有意思的几点：</p>
<p>其中 <code>MicroBatchExecution.scala</code> 继承自 <code>StreamExecution.scala</code> 抽象类，管理着 <code>Structured Streaming</code> 这种 <code>Micro-Batch</code> 方式的执行逻辑，与之对应还有 <code>Continuous</code> 方式，相应的子类是 <code>ContinuousExecution.scala</code>，关于 <code>Continuous</code> 方式这里不做详细描述。</p>
<p>阅读 <code>MicroBatchExecution.scala -&gt; runActivatedStream</code> 方法，其中有一段初始化逻辑如下：</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/microbatch-runstream.png" title="microbatch-runstream">
<p><code>currentBatchId</code> 默认为 -1，每次处理完一个 batch 后 +1，继续阅读 <code>populateStartOffsets</code> 方法</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/microbatch-populate.png" title="microbatch-populate">
<p><code>populateStartOffsets</code> 方法会用上一次 batch 的元数据更新下一次 batch 的 <code>SparkSession Config</code>，填充逻辑在 <code>OffsetSeqMetadata.setSessionConf(metadata, sparkSessionToRunBatches.conf)</code> 中</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/offsetseq-setconf.png" title="offsetseq-setconf">
<p>到此，配置不生效的原因已经清楚了：</p>
<p><strong>初次执行时没有设置该配置项，导致使用系统默认值 300，后续的 batch 每次都延续上一个 batch 的元数据，所以始终都会是 300</strong></p>
<p>查看下 <code>HDFS</code> 上元数据也印证了以上结论</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/streaming-meta.png" title="streaming-meta">
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>重新指定 <code>checkpoint</code> 路径，重新提交后配置生效，Tasks 数由 305 (5 + 300) 降至 7 (5 + 2)，Job 跑的如丝般顺滑 :)</p>
<img src="/2019/01/22/spark-sql-streaming-performance-profile/result.png" title="result">

    
	</div>
	<footer class="entry-footer">
		<div class="entry-meta-footer">
			<span class="category">
				
			</span>
			<span class="tags">
				
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL-Streaming-Performance/">Spark SQL, Streaming, Performance</a></li></ul>

			</span>
		</div>
	</footer>
	
    
<nav id="article-nav">
  
  
    <a href="/2018/10/22/spark-sql-streaming-monitor-alert/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          Spark Structured Streaming 实现监控告警
        
      </div>
    </a>
  
</nav>

  
</article>




    </div>
    <div class="mb-search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:datahacker.me">
  </form>
</div>
<footer id="footer">
	<h1 class="footer-blog-title">
		<a href="/">datahacker</a>
	</h1>
	<span class="copyright">
		&copy; 2019 huanzh<br>
		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	</span>
</footer>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div>
</body>
</html>